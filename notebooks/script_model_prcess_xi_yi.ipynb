{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Data Visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "import os\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"~/code/Agermita/intelligent_network_expansion/raw_data/data_finale_V4.csv\"\n",
    "\n",
    "processed_data = pd.read_csv(file_path, sep=',')\n",
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['Date'] = pd.to_datetime(processed_data['Date'], format='%Y-%m-%d')\n",
    "processed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y(df) ->np.array :\n",
    "    cells=df[[\"eNodeB identity\",'Cell ID','eNodeB_identifier_int']].sort_values(by='eNodeB_identifier_int')\n",
    "    cells=cells.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    data=[]\n",
    "    data_y=[]\n",
    "    for index, row in cells.iterrows():\n",
    "        df_cell=df[(df[\"eNodeB identity\"]==row[0]) & (df[\"Cell ID\"]==row[1])]\n",
    "        df_cell=df_cell.sort_values(by='eNodeB_identifier_int')\n",
    "        #df_cell=df_cell.reset_index(drop=True)\n",
    "        \n",
    "        #df_cell=replace_missing_dates(df_cell, start_date, end_date)\n",
    "        \"\"\" for modeling, the X and y should not contain sequences identty (eNodeB id and cell id)\"\"\"\n",
    "        df_cell.drop('eNodeB identity', axis=1, inplace=True)\n",
    "        df_cell.drop('Cell ID', axis=1, inplace=True)\n",
    "        df_cell.drop('Date', axis=1, inplace=True)\n",
    "        df_cell.drop('eNodeB_identifier_int', axis=1, inplace=True)\n",
    "        \n",
    "        \"\"\"---------------------\"\"\"\n",
    "        df_cell_y=df_cell['Trafic LTE.float']\n",
    "        df_cell.drop('Trafic LTE.float', axis=1, inplace=True)\n",
    "        \n",
    "        data.append(df_cell)\n",
    "        data_y.append(df_cell_y)\n",
    "\n",
    "    X=np.array(data)\n",
    "    y=np.array(data_y)\n",
    "    y=np.expand_dims(np.array(y), axis=2)\n",
    "    return cells, X , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get data related to each cell in an array of 3 dimensions (nb cells, nb days, columns)\n",
    "def get_cells_data(df) ->np.array :\n",
    "    cells=df[[\"eNodeB identity\",'Cell ID','eNodeB_identifier_int']].sort_values(by='eNodeB_identifier_int')\n",
    "    cells=cells.drop_duplicates()\n",
    "    \n",
    "    \n",
    "    data=[]\n",
    "    \n",
    "    for index, row in cells.iterrows():\n",
    "        df_cell=df[(df[\"eNodeB identity\"]==row[0]) & (df[\"Cell ID\"]==row[1])]\n",
    "        df_cell=df_cell.sort_values(by='eNodeB_identifier_int')\n",
    "        df_cell=df_cell.reset_index(drop=True)\n",
    "        \n",
    "        df_cell.drop('Trafic LTE.float', axis=1, inplace=True)\n",
    "        \n",
    "        data.append(df_cell)\n",
    "        \n",
    "\n",
    "    cells_data=np.array(data)\n",
    "    \n",
    "    return cells, cells_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells, cells_data=get_cells_data(processed_data)\n",
    "cell_0_data=pd.DataFrame(cells_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data_cell:pd.DataFrame,\n",
    "                     train_test_ratio: float,\n",
    "                     input_length: int) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"From a fold dataframe, take a train dataframe and test dataframe based on \n",
    "    the split ratio.\n",
    "    - df_train should contain all the timesteps until round(train_test_ratio * len(fold))\n",
    "    - df_test should contain all the timesteps needed to create all (X_test, y_test) tuples\n",
    "\n",
    "    Args:\n",
    "        fold (pd.DataFrame): A fold of timesteps\n",
    "        train_test_ratio (float): The ratio between train and test 0-1\n",
    "        input_length (int): How long each X_i will be : 3 month 90 days\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: A tuple of two dataframes (fold_train, fold_test)\n",
    "    \"\"\"\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    \n",
    "    # TRAIN SET\n",
    "    # ======================\n",
    "    last_train_idx = round(train_test_ratio * len(data_cell))\n",
    "    data_cell_train = data_cell.iloc[0:last_train_idx, :]\n",
    "\n",
    "    # TEST SET\n",
    "    # ======================    \n",
    "    first_test_idx = last_train_idx - input_length\n",
    "    data_cell_test = fold.iloc[first_test_idx:, :]\n",
    "\n",
    "    return (data_cell_train, data_cell_test)\n",
    "\n",
    "    # $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_cell0_train, data_cell0_test)=train_test_split(cell_0_data, 0.8, 90)\n",
    "# 90 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xi_yi(\n",
    "    data_cell:pd.DataFrame, \n",
    "    input_length:int, \n",
    "    output_length:int) -> Tuple[pd.DataFrame]:\n",
    "    \"\"\"given a fold, it returns one sequence (X_i, y_i) as based on the desired \n",
    "    input_length and output_length with the starting point of the sequence being chosen at random based\n",
    "\n",
    "    Args:\n",
    "        fold (pd.DataFrame): A single fold\n",
    "        input_length (int): How long each X_i should be --> 3 months\n",
    "        output_length (int): How long each y_i should be --> 1 month\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: A tuple of two dataframes (X_i, y_i)\n",
    "    \"\"\"\n",
    "    #drop unnecessary columns \"Date\", \"eNodeB identity\", \"Cell ID\", \"eNodeB_identifier_int\"  \n",
    "    data_cell.drop(data_cell.columns[[0, 1, 2, 8]], axis=1, inplace=True) \n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(data_cell) - (input_length + output_length) + 1\n",
    "    random_start = np.random.randint(first_possible_start, last_possible_start)\n",
    "    X_i = data_cell.iloc[random_start:random_start+input_length]  \n",
    "     \n",
    "    X_i.drop(X_i.columns[[0]], axis=1, inplace=True) # delete original trafic column\n",
    "    y_i = data_cell.iloc[random_start+input_length:\n",
    "                  random_start+input_length+output_length][[0]]\n",
    "    \n",
    "    return (X_i, y_i)\n",
    "    # $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(\n",
    "    cell_data:pd.DataFrame,\n",
    "    number_of_sequences:int,\n",
    "    input_length:int,\n",
    "    output_length:int) -> Tuple[np.array]:\n",
    "    \"\"\"Given a fold generate X and y based on the number of desired sequences \n",
    "    of the given input_length and output_length\n",
    "\n",
    "    Args:\n",
    "        fold (pd.DataFrame): Fold dataframe\n",
    "        number_of_sequences (int): The number of X_i and y_i pairs to include\n",
    "        input_length (int): Length of each X_i\n",
    "        output_length (int): Length of each y_i\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.array]: A tuple of numpy arrays (X, y)\n",
    "    \"\"\"\n",
    "    # $CHALLENGIFY_BEGIN    \n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(number_of_sequences):\n",
    "        (Xi, yi) = get_Xi_yi(cell_data, input_length, output_length)\n",
    "        X.append(Xi)\n",
    "        y.append(yi)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "    # $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN=9\n",
    "INPUT_LENGTH=90\n",
    "OUTPUT_LENGTH=30\n",
    "X_train, y_train = get_X_y(data_cell0_train, N_TRAIN, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "X_test, y_test = get_X_y(data_cell0_test, N_TEST, INPUT_LENGTH, OUTPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check if we have all days for each cell\n",
    "def cells_dim(df) ->np.array :\n",
    "    cells=df[[\"eNodeB identity\",'Cell ID','eNodeB_identifier_int']].sort_values(by='eNodeB_identifier_int')\n",
    "    cells=cells.drop_duplicates()    \n",
    "    cells_days=[]\n",
    "    for index, row in cells.iterrows():\n",
    "        df_cell=df[(df[\"eNodeB identity\"]==row[0]) & (df[\"Cell ID\"]==row[1])]\n",
    "        cells_days.append(df_cell.shape[0])\n",
    "    return cells_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_days=cells_dim(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(cells_days), max(cells_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells,X_cells,y_cells=create_X_y(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X,y, ratio=0.8) ->tuple:\n",
    "\n",
    "    X_train, X_test = np.split(X, [int(ratio*len(X))])\n",
    "    y_train, y_test = np.split(y, [int(ratio*len(y))])\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=split_train_test(X,y, ratio=0.8)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(input_shape: tuple, output_length) -> models:\n",
    "    #output_length = y_train.shape[1]\n",
    "    #input_shape =(X.shape[1],X.shape[2])\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    ## 1.1 - Recurrent Layer\n",
    "    model.add(layers.Masking(mask_value=-10, input_shape=input_shape))\n",
    "    \"\"\"model.add(layers.LSTM(units=64,\n",
    "                        activation='relu',\n",
    "                        return_sequences = True,\n",
    "                        kernel_regularizer=L1L2(l1=0.05, l2=0.05),\n",
    "                        ))\n",
    "    \"\"\"\n",
    "    model.add(layers.GRU(units=64,\n",
    "                        activation='relu',\n",
    "                        return_sequences = True\n",
    "                        ))\n",
    "    ## 1.2 - Hidden Dense Layers\n",
    "    \"\"\"\n",
    "    model.add(layers.Dense(32, activation=\"relu\", kernel_regularizer = L1L2(l1=0.05, l2=0.05)))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "    \"\"\"\n",
    "\n",
    "    ## 1.2 - Predictive Dense Layers\n",
    "\n",
    "    model.add(layers.Dense(output_length, activation='linear'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model: models) -> models:\n",
    "    \"\"\"\n",
    "    Compile the Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    # 2 - Compiler\n",
    "    # ======================\n",
    "    initial_learning_rate = 0.01\n",
    "\n",
    "    #lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=1000, decay_rate=0.5)\n",
    "\n",
    "    #adam = optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "    #model.compile(loss='mse', optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    model.compile(loss='mse', optimizer=\"adam\", metrics=['mae', 'mape'])\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "        model: models,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        patience=2,\n",
    "        validation_data=None, # overrides validation_split\n",
    "        validation_split_rate=0.3\n",
    "    ) -> Tuple[tensorflow.keras.Model, dict]:\n",
    "    \"\"\"\n",
    "    Fit the model and return a tuple (fitted_model, history)\n",
    "    \"\"\"\n",
    "     # $CHALLENGIFY_BEGIN\n",
    "    es = EarlyStopping(monitor = \"val_loss\",\n",
    "                      patience = patience,\n",
    "                      mode = \"min\",\n",
    "                      restore_best_weights = True)\n",
    "\n",
    "\n",
    "    history = model.fit(X, y,\n",
    "                        validation_split=validation_split_rate,    \n",
    "                        shuffle = False,\n",
    "                        batch_size = 32,\n",
    "                        epochs = 50,\n",
    "                        callbacks = [es],\n",
    "                        verbose = 1)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =(X_train.shape[1],X_train.shape[2])\n",
    "output_length = y_train.shape[1]\n",
    "input_shape, output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=initialize_model(input_shape, output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history=train_model(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        2,\n",
    "        None, # don't use validation data, use validation split rate\n",
    "        0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    # --- LOSS: MSE ---\n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('MSE')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    # --- METRICS:MAE ---\n",
    "\n",
    "    ax[1].plot(history.history['mae'])\n",
    "    ax[1].plot(history.history['val_mae'])\n",
    "    ax[1].set_title('MAE')\n",
    "    ax[1].set_ylabel('MAE')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_hist_predicted(y, y_pred, cells, start_date, end_date):\n",
    "    # start_date=processed_data['Date'].min()\n",
    "    # end_date=processed_data['Date'].max()\n",
    "    end_date_2=end_date+datetime.timedelta(days=y_pred.shape[1]) \n",
    "    dates=pd.date_range(start = start_date, end = end_date_2)\n",
    "    # reshape the y to the 2d\n",
    "    y_reshaped_2d=y\n",
    "    y_reshaped_2d=y_reshaped_2d.reshape(-1, y.shape[1])\n",
    "    # convert y and y_pred to Dataframe    \n",
    "    list_y_pred=pd.DataFrame(y_pred)\n",
    "    list_y=pd.DataFrame(y_reshaped_2d)\n",
    "    #add column to have distinction petween real trafic and predicted trafic\n",
    "    list_y_pred['period_trafic'] = pd.Series([\"predicted trafic\" for x in range(len(list_y_pred.index))])\n",
    "    list_y['period_trafic'] = pd.Series([\"real trafic\" for x in range(len(list_y.index))])\n",
    "    \n",
    "    cells=cells.reset_index(drop=True) # to be deleted after\n",
    "    #concatenate y and y_pred and cell ids\n",
    "    cell_data=pd.concat([list_y,list_y_pred],axis=1, ignore_index=True, sort=False)\n",
    "    cell_data_final=pd.concat([cells,cell_data],axis=1, sort=False)\n",
    "    # rename columns\n",
    "    columns=['eNodeB identity','Cell ID','eNodeB_identifier_int']\n",
    "    comunms_all=columns+(list(dates))\n",
    "    cell_data_final.columns=comunms_all\n",
    "    cell_data_final.set_index(['eNodeB identity','Cell ID','eNodeB_identifier_int'])\n",
    "    \n",
    "    # format the  dataframe to have dates as one column\n",
    "    cell_data_final2=cell_data_final.melt(id_vars=['eNodeB identity','Cell ID','eNodeB_identifier_int'], \n",
    "        var_name=\"Date\", \n",
    "        value_name=\"Trafic\")\n",
    "    \n",
    "    return cell_data_final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=processed_data['Date'].min()\n",
    "end_date=processed_data['Date'].max()\n",
    "df_trafic_predicted=data_hist_predicted(y, y_pred, cells, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"~/code/Agermita/intelligent_network_expansion/raw_data/data_finale_prediction.csv\"\n",
    "\n",
    "df_trafic_predicted.to_csv(file_path, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent_network_expansion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
