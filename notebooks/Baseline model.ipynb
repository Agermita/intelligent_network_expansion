{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Data Visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "import os\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"list of parameters\"\"\"\n",
    "N_FEATURES = 19\n",
    "N_TARGETS = 1\n",
    "\n",
    "FOLD_LENGTH = 8760\n",
    "FOLD_STRIDE = 728\n",
    "TRAIN_TEST_RATIO = 0.66\n",
    "\n",
    "N_TRAIN = 6666\n",
    "N_TEST = 3333\n",
    "INPUT_LENGTH = 112\n",
    "OUTPUT_LENGTH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path=\"~/code/Agermita/intelligent_network_expansion/raw_data/LTE KPIs Part 2.csv\"\n",
    "\n",
    "df_part_2 = pd.read_csv(file_path, sep = ';')\n",
    "df_part_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "processed_data.drop('Unnamed: 0.1', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(X,y, ratio=0.75) ->tuple:\n",
    "\n",
    "    X_train, X_test = np.split(X, [int(ratio*len(X))])\n",
    "    y_train, y_test = np.split(y, [int(ratio*len(y))])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def initialize_model(input_shape: tuple, output_length) -> models:\n",
    "    #output_length = y_train.shape[1]\n",
    "    #input_shape =(X.shape[1],X.shape[2])\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "     ## 1.1 - Recurrent Layer\n",
    "    model.add(layers.Masking(mask_value=0, input_shape=input_shape))\n",
    "    model.add(layers.LSTM(units=64,\n",
    "                          activation='relu',\n",
    "                          return_sequences = True,\n",
    "                          kernel_regularizer=L1L2(l1=0.05, l2=0.05),\n",
    "                          ))\n",
    "    ## 1.2 - Hidden Dense Layers\n",
    "\n",
    "    model.add(layers.Dense(32, activation=\"relu\", kernel_regularizer = L1L2(l1=0, l2=0.05)))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "    ## 1.2 - Predictive Dense Layers\n",
    "\n",
    "    model.add(layers.Dense(output_length, activation='linear'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model: models, params) -> models:\n",
    "    \"\"\"\n",
    "    Compile the Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    # 2 - Compiler\n",
    "    # ======================\n",
    "    initial_learning_rate = 0.01\n",
    "\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=1000, decay_rate=0.5)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=[\"mae\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "        model: models,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        patience=2,\n",
    "        validation_data=None, # overrides validation_split\n",
    "        validation_split=0.3\n",
    "    ) -> Tuple[models, dict]:\n",
    "    \"\"\"\n",
    "    Fit the model and return a tuple (fitted_model, history)\n",
    "    \"\"\"\n",
    "     # $CHALLENGIFY_BEGIN\n",
    "    es = EarlyStopping(monitor = \"val_loss\",\n",
    "                      patience = patience,\n",
    "                      mode = \"min\",\n",
    "                      restore_best_weights = True)\n",
    "\n",
    "\n",
    "    history = model.fit(X, y,\n",
    "                        validation_split = validation_split,\n",
    "                        shuffle = False,\n",
    "                        batch_size = 64,\n",
    "                        epochs = 50,\n",
    "                        callbacks = [es],\n",
    "                        verbose = 1)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = init_baseline()\n",
    "baseline_score = baseline_model.evaluate(X_test, y_test)\n",
    "print(f\"- The Baseline MAE on the test set is equal to {round(baseline_score[1],2)} Celsius degrees\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent_network_expansion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
