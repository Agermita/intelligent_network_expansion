{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow\\nfrom typing import Dict, List, Tuple, Sequence\\n\\nfrom tensorflow.keras import models\\nfrom tensorflow.keras import layers\\nfrom tensorflow.keras import optimizers, metrics\\nfrom tensorflow.keras.regularizers import L1L2\\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\\nfrom sklearn.model_selection import train_test_split\\nfrom keras.layers import LSTM\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Data Visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# System\n",
    "import os\n",
    "\n",
    "# Deep Learning\n",
    "\"\"\"\n",
    "import tensorflow\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read file you saved\n",
    "file_path_X_test=\"~/code/Agermita/intelligent_network_expansion/raw_data/X_test_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(file_path_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39999, 90)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mImpossible d’exécuter le code ; la session a été supprimée. Essayez de redémarrer le noyau."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "dff= dd.from_pandas(df,npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_reshaped\u001b[39m=\u001b[39m(\u001b[39mlist\u001b[39m((\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin((\u001b[39mstr\u001b[39m(df[:][i])\u001b[39m.\u001b[39mstrip(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m[]\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39msplit()))\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(df))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "df_reshaped=(list((\" \".join((str(df[:][i]).strip(\"\\n[]\")).split())).split(\" \")) for i in range(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m new_df\u001b[39m=\u001b[39m[]\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m dff\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m      3\u001b[0m     list_val\u001b[39m=\u001b[39m[]\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m index, value \u001b[39min\u001b[39;00m row\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dff' is not defined"
     ]
    }
   ],
   "source": [
    "new_df=[]\n",
    "for index, row in dff.iterrows():\n",
    "    list_val=[]\n",
    "    for index, value in row.items():\n",
    "        new_list=list((\" \".join((str(value).strip(\"\\n[]\")).split())).split(\" \"))\n",
    "        \n",
    "        list_val.append(new_list)\n",
    "    new_df.append(list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<generator object <genexpr> at 0x7f1887c26110>,\n",
       "       <generator object <genexpr> at 0x7f1887c259a0>,\n",
       "       <generator object <genexpr> at 0x7f1887c26180>, ...,\n",
       "       <generator object <genexpr> at 0x7f1873b15230>,\n",
       "       <generator object <genexpr> at 0x7f1873b152a0>,\n",
       "       <generator object <genexpr> at 0x7f1873b15310>], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=np.array(new_df)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_changed=[]\n",
    "for test in df:\n",
    "  new_list=list((\" \".join((data.strip(\"\\n[]\")).split())).split(\" \") for data in test)\n",
    "  print (len(new_list))\n",
    "  X_test_changed.append(new_list)\n",
    "#X_test_final=np.array(X_test_changed)\n",
    "#X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(read_X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    # --- LOSS: MSE ---\n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('MSE')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    # --- METRICS:MAE ---\n",
    "\n",
    "    ax[1].plot(history.history['mae'])\n",
    "    ax[1].plot(history.history['val_mae'])\n",
    "    ax[1].set_title('MAE')\n",
    "    ax[1].set_ylabel('MAE')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(input_shape: tuple, output_length) -> models:\n",
    "    #output_length = y_train.shape[1]\n",
    "    #input_shape =(X.shape[1],X.shape[2])\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    \"\"\"\n",
    "\n",
    "    model = models.Sequential()\n",
    "    ## 1.1 - Recurrent Layer\n",
    "    model.add(layers.Masking(mask_value=-10, input_shape=input_shape))\n",
    "\n",
    "    model.add(layers.LSTM(units=64,\n",
    "                        activation='relu',\n",
    "                        return_sequences = True,\n",
    "                        kernel_regularizer=L1L2(l1=0.05, l2=0.05),\n",
    "                        ))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(32, activation='sigmoid', return_sequences=False))\n",
    "    ## 1.2 - Hidden Dense Layers\n",
    "\n",
    "    model.add(layers.Dense(32, activation=\"relu\", kernel_regularizer = L1L2(l1=0.05, l2=0.05)))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "\n",
    "    ## 1.2 - Predictive Dense Layers\n",
    "\n",
    "    model.add(layers.Dense(output_length, activation='linear'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model: models) -> models:\n",
    "    \"\"\"\n",
    "    Compile the Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    # 2 - Compiler\n",
    "    # ======================\n",
    "    initial_learning_rate = 0.01\n",
    "\n",
    "    #lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=1000, decay_rate=0.5)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "    #model.compile(loss='mse', optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "        model: models,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        patience=2,\n",
    "        validation_data=None, # overrides validation_split\n",
    "        validation_split_rate=0.3\n",
    "    ) -> Tuple[tensorflow.keras.Model, dict]:\n",
    "    \"\"\"\n",
    "    Fit the model and return a tuple (fitted_model, history)\n",
    "    \"\"\"\n",
    "     # $CHALLENGIFY_BEGIN\n",
    "    es = EarlyStopping(monitor = \"val_loss\",\n",
    "                      patience = patience,\n",
    "                      mode = \"min\",\n",
    "                      restore_best_weights = True)\n",
    "\n",
    "\n",
    "    history = model.fit(X, y,\n",
    "                        validation_split=validation_split_rate,\n",
    "                        shuffle = False,\n",
    "                        batch_size = 32,\n",
    "                        epochs = 50,\n",
    "                        callbacks = [es],\n",
    "                        verbose = 1)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =(X_train.shape[1],X_train.shape[2])\n",
    "output_length = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape =(90,26)\n",
    "output_length = 30\n",
    "model=initialize_model(input_shape, output_length)\n",
    "model.summary()\n",
    "model=compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on one cell data\n",
    "model, history=train_model(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    2,\n",
    "    None, # don't use validation data, use validation split rate\n",
    "    0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nwexamples = []\n",
    "for row in read_X_train:\n",
    "    nwrow = []\n",
    "    for r in row:\n",
    "        nwrow.append(eval(r))\n",
    "    nwexamples.append(nwrow)\n",
    "print(nwexamples)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent_network_expansion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
